# Papers to read and review
 

[CONSISTENCY BASED UNSUPERVISED SELF-TRAINING FOR ASR PERSONALISATION](https://arxiv.org/pdf/2401.12085.pdf)

[STREAMING BILINGUAL END-TO-END ASR MODEL USING ATTENTION OVER
MULTIPLE SOFTMAX](https://arxiv.org/pdf/2401.11645.pdf)

[KEEP DECODING PARALLEL WITH EFFECTIVE KNOWLEDGE DISTILLATION FROM
LANGUAGE MODELS TO END-TO-END SPEECH RECOGNISERS](https://arxiv.org/pdf/2401.11700.pdf)

[EXTREME ENCODER OUTPUT FRAME RATE REDUCTION: IMPROVING
COMPUTATIONAL LATENCIES OF LARGE END-TO-END MODELS](https://arxiv.org/pdf/2402.17184.pdf)

[Dynamic Encoder Size Based on Data-Driven Layer-wise Pruning for Speech
Recognition](https://arxiv.org/pdf/2407.18930)

[On the Effect of Purely Synthetic Training Data
for Different Automatic Speech Recognition Architectures](https://arxiv.org/pdf/2407.17997)

[Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment](https://arxiv.org/abs/2407.17716)

[Explaining Spectrograms in Machine Learning: A Study on Neural Networks for Speech Classification](https://arxiv.org/abs/2407.17416)

[SC-MoE: Switch Conformer Mixture of Experts for Unified Streaming and
Non-streaming Code-Switching ASR](https://arxiv.org/pdf/2406.18021)

[TOKEN-WEIGHTED RNN-T FOR LEARNING FROM FLAWED DATA](https://arxiv.org/pdf/2406.18108)

[FROM SPARSE TO SOFT MIXTURES OF EXPERTS](https://arxiv.org/pdf/2308.00951)

[Speech Prefix-Tuning with RNNT Loss for Improving LLM Predictions](https://arxiv.org/pdf/2406.14701)

[InterBiasing: Boost Unseen Word Recognition through Biasing Intermediate
Predictions](https://arxiv.org/pdf/2406.14890)

[AudioBench: A Universal Benchmark for Audio Large Language Models](https://arxiv.org/pdf/2406.16020)

[Exploring the Capability of Mamba in Speech Applications](https://arxiv.org/pdf/2406.16808)

[Contextualized End-to-end Automatic Speech Recognition with
Intermediate Biasing Loss](https://arxiv.org/pdf/2406.16120)

[Decoder-only Architecture for Streaming End-to-end Speech Recognition](https://arxiv.org/pdf/2406.16107)

[Fusing Audio and Metadata Embeddings
Improves Language-based Audio Retrieval](https://arxiv.org/pdf/2406.15897)

[Efficiently Train ASR Models
that Memorize Less and Perform Better with Per-core Clipping](https://arxiv.org/pdf/2406.02004v1)

[Streaming Decoder-Only Automatic Speech Recognition with Discrete Speech
Units: A Pilot Study](https://arxiv.org/pdf/2406.18862)

[LARGE-SCALE CONTRASTIVE LANGUAGE-AUDIO PRETRAINING WITH
FEATURE FUSION AND KEYWORD-TO-CAPTION AUGMENTATION](https://arxiv.org/pdf/2211.06687)