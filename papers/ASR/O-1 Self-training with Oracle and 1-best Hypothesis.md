# O-1: Self-training with Oracle and 1-best Hypothesis

[https://arxiv.org/pdf/2308.07486.pdf](https://arxiv.org/pdf/2308.07486.pdf)

Исследователи из гугла предлагают альтернативу MBR-обучению. Мотивация заключается в желании побуждать модель поднимать в топ n-best Oracle гипотезу и обучаться на нее. Говорят (и, наверное, не врут) что до них таких подходов не было. Также они критикуют вычислительно дорогой MBR и делают свой подход более экономным.

В работе используют RNN-T модели и rnnt-loss.

Для начала про MBR-training. 

Его мотивация такая: давайте будем обучаться не на минимизацию вероятности истинной гипотезы, а на минимизацию взвешенного по вероятностям WER-а всех возможных гипотез (на практике берут n-best list). Лосс выглядит следующим образом:

![Untitled](O-1%20Self-training%20with%20Oracle%20and%201-best%20Hypothesis/Untitled.png)

Здесь $\mathcal{Y}$ есть N-best list, $\hat{p}(Y_i | X)$ - вероятность $i$ - ой гипотезы в N-best (считается софтмаксом по вероятностям всех гипотез в этом списке), $W(Y_i, Y^*)$  - **WER** между гипотезой и референсом.  Для получения n-best list надо прогнать бимсерч + потом еще надо посчитать сам лосс, что ощутимо дороже обычного ASR-лосса (ctc, rnnt, las).

## O-1 Training Procedure

Авторы предлагают обучаться на следующий лосс:

$$
\mathcal{L}_{O1} = -\log p(Y_{oracle} | X) \cdot (1 - \operatorname{WER}(Y_{oracle}, Y^*)) +
$$

$$
+ 

\log p(Y_{1-best} | X) \cdot \operatorname{WER}(Y_{1-best}, Y^*)
$$

Как это работает:

1) При обучении rnn-t после forward-а прогоняем бимсерч и извлекаем n-best list

2) Считаем WER каждой гипотезы из n-best

3) Гипотезу с минимальным WER обозначаем $Y_{orcale}$, самую вероятную гипотезу обозначаем $Y_{1-best}$

4) Считаем $\log p(Y_{i,j} | X)$ как RNN-T лосс для $i = oracle$ и $j = 1-best$.

5) Скэйлим получившиеся RNN-T лоссы в соответвствии с формулой выше

6) Когда WER для $1-best$  и $oracle$ одинаков, $\mathcal{L}_{O1} \rightarrow 0$. Чтобы не пострадать от этого, в обучении также используем вспомагательный RNN-T лосс.

С помощью такого алгоритма мы заставляем модель тащить наверх $oracle$ - гипотезу, а также избавляемся от необходимости сложного подсчета MBR-лосса с помощью отбрасывания всех гипотез, кроме двух.

Как отмечено в 6), в экспериментах c MBR и O-1 авторы используют следующие лоссы:

$$
\mathcal{L}'_{embr} = \mathcal{L}_{embr} + \gamma\mathcal{L}_{rnnt}
$$

$$
\mathcal{L}'_{O1} = \mathcal{L}_{O1} + \lambda\mathcal{L}_{rnnt}
$$

$$
\mathcal{L}_{CTC} = P(\text{target} | \text{audio})
$$

$$
P(\text{target} | \text{audio}) = \sum_{alignment}P(\text{alignment} | \text{audio})
$$

## O-1 Unsupervised training

Здесь все просто. Используют hard distillation (обучается на данных, размеченных жирной моделью), и в свой лосс подставляют псевдолейблы, вместо таргета.

$$
\mathcal{L}_{distill} = \mathcal{L}_{rnnt}(X, Y')
$$

$$
\mathcal{L}_{O1\_distill} = \mathcal{L}_{O1}(X, Y, Y') + \lambda\mathcal{L}_{distill}
$$

## Datasets

Используют два сетапа данных. Первый - SpeechStew, на котором обучают модели supervised. Второй - внутренний датасет, состоящий из коротких запросов к ассистенту + 300К часов разных данных типа Dictation, YouTube, Telephony. 

Там у них есть достаточное количество неразмеченных данных (500M, ~400K часов) короких поисковых запросов, поэтому проводят эксп еще и с ними.

## Models

Во всех экспах используют RNN-T, где энкодер - конформер, декодер - LSTM. Для экспериментов со SpeechStew берут две модели, на 100М и на 600M параметров. Для экспериментов на внутренних данных используют Hybrid Autoregressive Transducer.

# Эксперименты

## O-1 Supervised Self Training

Обучают модели только на размеченных данных с ванильным RNN-T лоссом, EMBR-лоссом и O-1 лоссом. Модель на 100М параметров обучается с нуля, модель на 600М параметров обучается после wav2vec2 - претрейна на LibriLight. Модель обучается до сходимости (~300K итераций) и потом дообучается с помощью EMBR / O-1 еще 100К шагов. Коэффициенты RNN-T лосса в $\mathcal{L'}_{O-1}$  и $\mathcal{L'}_{EMBR}$ берут 0.1. Во всех экспах используют нормализацию по длине таргета в RNN-T лоссе.

## O-1 Unsupervised Learning

Берут non-causal модель-учитель с контекстом вправо 64 и стриминговую модель-студента. (почему?..)
Для бэйзлайна модель-учителя обучают на размеченных данных, модель-студента на размеченных + псевдолейблах от учителя.  Дальше что-то стремное у них написано. Обучают совместно учителя и студента с помощью hard-distillation до сходимости (причем написано, что это CrossEntropy training). Потом дообучают это дело с EMBR. 
Для O-1 training не используют CE-training, а обучают сразу на O-1 loss. 
В каждом батче у них одинаковое количество размеченных данных и псевдолейблов. Бимсерч для MBR берут с размером 8 как для обучения, так и для инференса.
Бимсерч для обучения O-1 берут с размером 18 на SpeechStew и с размером 8 на своих данных. На инференсе берут размер 8 чтоб честно сравнивать с EMBR.

# Результаты

В первом эксперименте O-1 дал значимое улучшение везде. EMBR везде оказался хуже O-1, а на некоторых хуже бейзлайна RNN-T, с которого он доучивался.

![Untitled](O-1%20Self-training%20with%20Oracle%20and%201-best%20Hypothesis/Untitled%201.png)

Во втором эксперименте получились следующие результаты:

![Untitled](O-1%20Self-training%20with%20Oracle%20and%201-best%20Hypothesis/Untitled%202.png)

O-1 снова показал значимый прирост качества.

Помимо табличек авторы приводят интересные графики.

На первом отложили 1-best EMBR, 1-best O-1  и Oracle WER на тесте либриспича. Видно, что O-1 training намного лучше справляется с поднятием лучшей гипотезы в начало n-best list. Oracle-WER считали для бэйзлайна.

![Untitled](O-1%20Self-training%20with%20Oracle%20and%201-best%20Hypothesis/Untitled%203.png)

На следующей картинке изображены 2 графика.

![Untitled](O-1%20Self-training%20with%20Oracle%20and%201-best%20Hypothesis/Untitled%204.png)

- На первом - средний WER на всех тест-сетах SpeechStew во время обучения разных моделей с разными beam-size. Думаю WER здесь считался от 1-best. Видно, что увеличение beam_size сильно бустит O-1.
- На втором - производительность моделей в зависимости от числа гипотез, вовлеченных в подсчет лосса. При этом beam-size здесь фиксирован. Авторы говорят, что EMBR показывает наилучшие результаты при #hyps=8 т.к. oracle-гипотеза чаще всего находится на 5-8 позиции. O-1 же по своей конструкции использует максимум 2 гипотезы для подсчета лосса.

# Итоги

Интересный и рабочий подход, надо пробовать. Стабильно улучшает WER.