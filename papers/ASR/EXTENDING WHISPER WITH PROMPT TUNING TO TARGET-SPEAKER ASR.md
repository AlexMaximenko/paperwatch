# EXTENDING WHISPER WITH PROMPT TUNING TO TARGET-SPEAKER ASR

[https://arxiv.org/pdf/2312.08079.pd](https://arxiv.org/pdf/2312.08079.pdf)f

![Screenshot 2024-01-19 at 14.20.23.png](EXTENDING%20WHISPER%20WITH%20PROMPT%20TUNING%20TO%20TARGET-SPEAKER%20ASR/Screenshot_2024-01-19_at_14.20.23.png)

Коллеги из Китая и США предлагают “инновационный” подход по адаптации Виспера на задачу TS-ASR. Подход заключается в промп-тюнинге энкодера и декодера + в добавлении speaker-эмбеддинга в декодер. Так же говорят, что у них получилось сохранить крутые свойства виспера типа денормализации / таймстемпов.

### Whisper

Модель для дистилляции - Whisper - жирная Encoder-Decoder модель, состоящая из трансформерных слоев (+ у энкодера еще есть свертки перед ними), которая обучалась на огромном количестве Weakly-Supervised данных сразу на несколько задач:

- English ASR
- Any-to-English speech translation
- Non-English ASR
- VAD

![Untitled](DISTIL-WHISPER/Untitled.png)

Больше всего в обучении было English ASR данных, причем с огромным отрывом.

![Untitled](DISTIL-WHISPER/Untitled%201.png)

### Target-Speaker ASR

Задача TS-ASR заключается в распознавании речи одного определенного спикера среди, возможно, говорящих с ним одновременно людей. Для этого модель принимает на вход аудиодорожку для распознавания и какой-то артефакт спикера: заранее выученный эмбеддинг / аудио, где говорит только он.

![Screenshot 2024-01-19 at 14.31.42.png](EXTENDING%20WHISPER%20WITH%20PROMPT%20TUNING%20TO%20TARGET-SPEAKER%20ASR/Screenshot_2024-01-19_at_14.31.42.png)

В своей работе авторы используют [X-vectors](https://danielpovey.com/files/2018_icassp_xvectors.pdf), посчитанные на 15 секундах звука target-speaker-а.

### Инновационный метод

![Screenshot 2024-01-19 at 15.09.16.png](EXTENDING%20WHISPER%20WITH%20PROMPT%20TUNING%20TO%20TARGET-SPEAKER%20ASR/Screenshot_2024-01-19_at_15.09.16.png)

Суть работы описывается так:

1. Давайте засунем в энкодер эмбеддинг спикера + обучаемые векторы-промпты
2. Давайте еще и в декодер засунем обучаемые векторы-промпты
3. А еще круче, если перед входом в модель прогонять эти векторы-промпты через MLP + residual connection
4. Ну и если даже этого мало, то сделаем **DEEP** Prompting - это когда добавялем промпты на каждый слой энкодера и декодера
5. А теперь сравнимся с LORA-ой и …

Другими словами, Whisper принимает на вход спектрограмму $X$, переводит это в скрытое состояние $H$ и затем использует его для декодинга. В формулах это выглядит так:

$$
H = \operatorname{AudioEncoder}_{\phi_e}(Conv(X))

$$

$$
\hat{y}_t = \operatorname{TextDecoder}_{\phi_d} (g, \hat{y}_{1:t-1}, H)
$$

Здесь $\phi$ - параметры энкодера и декодера соответственно, $g$ - промпт для виспера, например по дефолту для распознавания английской речи без таймстемпов он будет состоять из следующих токенов: [⟨|prev|⟩, prev-text, ⟨|start-of-transcribe|⟩, ⟨|EN|⟩,
⟨|transcribe|⟩, ⟨|no-timestamps|⟩], где prev-text отвечает за обуславливание на контекст.

Теперь в этих обозначениях запишем идею авторов:

Эмбеддинг спикера извлекаем внешней моделью, получаем вектор $e_i$. Далее пропускаем его через линейный слой $\textbf{W}$, достаем из рюкзака обучаемые векторы-промпты энкодера $\textbf{P}_e$ и конкатим их с выходом сверток перед трансформерными слоями в энкодере:

$$
H_i = \operatorname{AudioEncoder}_{\phi_e}([\textbf{W}e_i, \textbf{P}_e, Conv(X)])

$$

Дальше засунем получившееся состояние в декодер, добавив ему в промт вместо prev-text наши обучаемые векторы:

$$
\hat{y}_t = \operatorname{TextDecoder}_{\phi_d} (g(\textbf{P}_d), \hat{y}_{i,1:t-1}, H)
$$

Если хотим **DEEP PROMPTING,** то нам еще надо на каждом трансформерном слое заменять аутпуты промптов с предыдущего слоя на новые обучаемые промпт-векторы.

## Experiments

Эксперименты ставили на датасете **Libri2Mix,** который состоит из замешанных аудио LibriSpeech-а:

1. Берем аудио 1
2. Берем аудио 2 от другого спикера
3. Мешаем со случайным уровнем громкости + добавляем шум, если нужно

Обучались на сетах train-100-clean и train-100-both, где в первом - шума нет, во втором - есть (WHAM, шумы типа coffee shop, restaraunt, bars, … in San Francisco Bay Area).

Тестировались на dev-clean, test-clean, dev-noise и test-noise сетах.

Сначала они провели ablation чтоб посмотреть, нужны ли MLP-проекции обучаемых промптов + нужно ли их делать раздельными на каждом слое. Результаты на табличке ниже. Общий вывод - проекции в целом помогают, но только если делать их отдельно для каждого слоя. Число векторов в промпте: скорее всего чем их больше, тем лучше результат. Но это прям очень нестрого, судя по результатам. Для дальнейших экспериментов авторы взяли 16 векторов в промпте.

![Screenshot 2024-01-19 at 15.42.43.png](EXTENDING%20WHISPER%20WITH%20PROMPT%20TUNING%20TO%20TARGET-SPEAKER%20ASR/Screenshot_2024-01-19_at_15.42.43.png)

Теперь они предлагают сравниться с другими моделями. Сравниваться предлагают по WER-у и по числу обучаемых параметров.

![Screenshot 2024-01-19 at 15.47.46.png](EXTENDING%20WHISPER%20WITH%20PROMPT%20TUNING%20TO%20TARGET-SPEAKER%20ASR/Screenshot_2024-01-19_at_15.47.46.png)

В целом на верхние модели можно не смотреть, информативно здесь сравниваться с LORA-ой. И объективно - метод авторов работает значительно хуже по WER-у.

Да, на инференсе у них в полтора раза меньше параметров, по сравнению с LORA-ой, но это незаметно из-за размеров самого виспера. Зато во время обучения из-за их MLP-проекций обучаемых параметров сильно больше.

Также приводят полный файнтюнинг маленьких Whisper-ов и показывают, что это работает намного хуже, чем файнтюнинг с адаптерами и prompt-tuning-ом.

В конце приводят ablation, в котором можно разглядеть влияние всех их идей на итоговое качество модели:

![Screenshot 2024-01-19 at 15.52.38.png](EXTENDING%20WHISPER%20WITH%20PROMPT%20TUNING%20TO%20TARGET-SPEAKER%20ASR/Screenshot_2024-01-19_at_15.52.38.png)

Из познавательного - тюнить лучше энкодер, это дает намного больший буст метрик, что логично - подстраивание под аудио-домен более важно, чем под текстовый.

И еще одна фича - сохранение приколюх виспера типа пунктуации/денормализации/таймстемпов: авторы пишут, что если делать файнтюн на чистом тексте LibriSpeech-а, то модель забывает все эти свои свойства. Поэтому они делают файнтюн на псевдолейблах виспера без таймстемпов (именно псевдолейблах, не forced-decoding), что позволяет сохранить эти качества.

Интересный результат - если файнтюнить виспер с промптом <no_timestamps> и потом убрать его на инференсе, модель сможет расставлять таймстемпы. По крайней мере они так пишут, качество не замеряли.

Вывод - больше не выбирать статьи, авторы которых учатся в школе.