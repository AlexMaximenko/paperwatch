# DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models

[https://arxiv.org/pdf/2305.17651.pdf](https://arxiv.org/pdf/2305.17651.pdf)

Авторы предлагают эффективный task-agnostic метод прунинга и дистилляции self-supervised предобученных трансформерных моделей. Для этого пробуют подходы с дистилляцией из DistillHuBERT и FitHuBERT и подход с structured прунингом, который будет описан ниже.

# Что такое SSL-претрейн

По сути, это есть перенос идеи BERT на аудио-домен. Самые известные реализации этого подхода - wav2vec2.0, HuBERT и WavLM. Общая идея всех подходов - предобучить трансформерный энкодер на неразмеченных аудио-данных с помощью похожей на BERT задачи. Все подходы используют одну и ту же архитектуру сетки - сверточные слои, за которыми следуют слои трансформеров. Разница в постановках задачи обучения этих моделей не важна для понимания статьи, ее опущу.
Также для всех этих моделей общим является большое количество компьюта, необходимое для обучения. Авторы обычно приводят несколько версий моделей, от маленьких (~100M параметров) до огромных (~1B параметров). Модельки жирные и не подходят для прода. Хотим перенести знания из них в менее жирные модели. Как делать это эффективно - читать далее.

# DistillHuBERT

В этой работе авторы предлагают

1. Ужать модель-студента по глубине - берут только сверточные слои и два траснформерных слоя. Инициализируют все это дело свертками и первыми двумя слоями модели-учителя.
2. Обучать модель не только на выход последнего слоя модели-учителя, а прикрутить к своей модели prediction heads, которые будут учиться на промежуточные слои модели-учителя.

![Untitled](DPHuBERT%20Joint%20Distillation%20and%20Pruning%20of%20Self-Supervised%20Speech%20Models/Untitled.png)

Дистилляцию проводят на 960 часах Либриспича, не используя разметку. Результаты смотрят на бенчмарке SUPERB, который содержит в себе задачи:  

- phoneme recognition (PR)
- keyword spotting (KS)
- intent classification (IC)
- speaker identification (SID)
- emotion recognition (ER)
- automatic speech recognition (ASR)
- query by example spoken term detection (QbE)
- slot filling (SF)
- automatic speaker verification (ASV)
- and speaker diarization (SD)

Бенчмарк внутри себя берет модель, замораживает ее и навешивает дополнительные слои, соответствующие каждой задаче. После этого файнтюнит на тренировочных данных, соответствующих каждой задаче и замеряет результаты на тестовом сете.

Получают следующие результаты:

![Untitled](DPHuBERT%20Joint%20Distillation%20and%20Pruning%20of%20Self-Supervised%20Speech%20Models/Untitled%201.png)

Здесь следует смотреть на следующие строки:

- HuBERT - модель-учитель, с которой авторы дистиллируются. Она здесь лучшая на всех задачах
- predict last layer - бэйзлайн, в котором модель дистиллируется только на выход последнего слоя учителя
- predict w/ hidden - бэйзлайн, в котором модель дистиллируется не с помощью навешивания prediction head, а с помощью прямой минимизации лосса (L1 - cosine similarity) между слоями студента и слоями учителя
- w/ prediction heads - из модели не удаляют prediction heads. В бенчмарке обучаемо-взешенно суммируются выходы голов и последнего слоя.
- DistillHuBERT - модель авторов
- wav2vec - берут маленький wav2vec (кажется, даже не wav2vec2.0)

Как видим, подход авторов бьет бэйзлайны и сравнимый по размеру wav2vec. Также видим что оставлять головы полезно.

Также авторы приводят несколько интересных графиков/таблиц

![Untitled](DPHuBERT%20Joint%20Distillation%20and%20Pruning%20of%20Self-Supervised%20Speech%20Models/Untitled%202.png)

Здесь они проводили ablation того, на сколько слоев учителя стоит обучать студента. Видим, что выбранные ими слои получают в среднем задачам лучшие результаты, но для каждой конкретной таски можно найти вариант получше. Так, WER при обучении только на 8 слой получается значимо лучше.

Дальше они изучают значимость каждого слоя для разных задач.

![Untitled](DPHuBERT%20Joint%20Distillation%20and%20Pruning%20of%20Self-Supervised%20Speech%20Models/Untitled%203.png)

Для этого обучают модель с 12-ю головами и потом файтюнят на каждую задачу. Во время файнтюна складывают выход сверточного слоя, выход из последнего слоя трансформера и выходы каждой из голов. Суммируют взвешенно, веса обучаются во время файнтюна. После этого берут веса, нормируют их к 1 и строят график выше. Видно, что для разных задач полезны разные слои. Выход из трансформера очень полезен для всех задач, особенно для SpeakerIdentification (вклад вносит он один). Для ASR также полезны головы на 7-10 слоях.

# FitHuBERT

В этой работе авторы предлагают другую схему дистилляции. Что предлагают авторы:

- Оставить столько же слоев, сколько и в учителе, ужав модель по ширине
- Уменьшить количество каналов в первых сверточных слоях и добавить pointwise свертки
- Добавить time reduction (свертка со страйдом)
- Навешивать все prediction heads не на последний слой студента, а на каждый слой по одной

Во время файнтюна все головы, кроме последней, убирались.

![Untitled](DPHuBERT%20Joint%20Distillation%20and%20Pruning%20of%20Self-Supervised%20Speech%20Models/Untitled%204.png)

Результаты показывают, что результативность в сравнении с DistillHuBERT зависит от задачи. Я привык смотреть на WER, и у FitHuBERT результат значимо лучше.

![Untitled](DPHuBERT%20Joint%20Distillation%20and%20Pruning%20of%20Self-Supervised%20Speech%20Models/Untitled%205.png)

Также авторы провели ablation, в котором исследовали необходимое количество prediction heads.

![Untitled](DPHuBERT%20Joint%20Distillation%20and%20Pruning%20of%20Self-Supervised%20Speech%20Models/Untitled%206.png)

# Подход авторов

Используют два этапа обучения:

1. В первом учат модель на дистилляцию + прунинг
2. Обрезанную после первого этапа модель доучивают только на дистилляцию

### Дистилляция

В своей работе авторы предлагают объединить дистилляцию и прунинг. Дистилляцию берут из FitHuBERT - берут студента такой же глубины и на каждый слой из выбранных довешивают prediction head, выход которого приближает соответствующий слой модели-учителя по L1-лоссу + косинусной близости. 

Формально, обзовем $X^{tea}_i$  и $X^{stu}_i$  - выходы $i-$ого слоя учителя и студента соответственно. а $W_i$ - prediction head на $i-$ом слое. Тогда лосс будет следующим:

$$
\mathcal{L}^{dis} = \sum\limits_{i \in \mathcal{S}}\mathcal{L}(X^{tea}_i, X^{stu}_i, W_i)
$$

где $\mathcal{S}$ - набор слоев, которые мы будем приближать.

### Structural pruning

Авторы используют следующий подход. Обозначим параметры модели-студента $\theta = \{\theta_j\}_{j=1}^n$, где каждый $\theta_j$ - группа параметров (каналы сверток, головы Attention-а, промежуточные представления FFN). Чтобы выбрать, какие группы нам нужны, а какие можно вырезать, введем булевую масочку $z = \{z_j\}_{j=1}^n$ и при обучении будем домножать веса модели-студента на эту масочку, получая $\hat{\theta} = \theta \cdot z$. Также в лосс добавляется дополнительное слагаемое, $L_0$ - норма $\hat{\theta}$, то есть количество ненулевых элементов в маске $z$. Вводим распределение на $z$ с распределением $q(z; \alpha)$ с параметрами $\alpha$.

Лосс для первого этапа обучения выглядит следующим образом:

$$
\min\limits_{\theta, \alpha} \mathbb{E}_{z \sim q} [ \frac{1}{D} \sum \limits_{k=1}^{D} \mathcal{L}^{dis}(f^{tea}(x_k), f^{stu}(x_k; \hat{\theta})) + \lambda \|\hat{\theta}\|_0 ]
$$

Чтобы сделать лосс дифференцируемым, используем reparametrization trick с Hard Concrete (твердобетонным) распределением:

![Untitled](DPHuBERT%20Joint%20Distillation%20and%20Pruning%20of%20Self-Supervised%20Speech%20Models/Untitled%207.png)

Здесь $l < 0$ и $r > 0$ - константы, которые растягивают область значений до $[l, r]$, после чего эта область обрезается до $[0, 1]$, что позволяет занулить довольно большую часть значений. Зная это распределение, можем посчитать матож из лосса и выразить его как фнукцию от $\alpha$, сделав его дифференцируемым.

![Untitled](DPHuBERT%20Joint%20Distillation%20and%20Pruning%20of%20Self-Supervised%20Speech%20Models/Untitled%208.png)

Проделав это, авторы решают поменять задачу и обучаться не на лосс, приведенный выше, а вынести регуляризационный член как условие и переформулировать задачу с помощью метода множителей Лагранжа. Итого, финальный лосс для первого этапа обучения:

![Untitled](DPHuBERT%20Joint%20Distillation%20and%20Pruning%20of%20Self-Supervised%20Speech%20Models/Untitled%209.png)

В коде никакого максимума у них нет, просто минимизация. 

## Эксперименты

Используют 960 либо 100 часов Либриспича (без разметки). На первом этапе учат 50к шагов, на втором - 25к шагов. Замеряют на SUPERB.

Главная табличка с результатами ниже

![Untitled](DPHuBERT%20Joint%20Distillation%20and%20Pruning%20of%20Self-Supervised%20Speech%20Models/Untitled%2010.png)

Видно заметное преимущество в сравнении с предыдущими способами дистилляции: DistillHuBERT, FitHuBERT. 12-Layer Half здесь есть дистилляция HuBERT-а в модель с тем же количеством слоев, но с уменьшенной в 2 раза внутренней размерностью.

Приводят график, показывающий прунинг на разных слоях модели.

![Untitled](DPHuBERT%20Joint%20Distillation%20and%20Pruning%20of%20Self-Supervised%20Speech%20Models/Untitled%2011.png)

Видно, что в сверточных сетях больше всего лишних каналов в первом и в последнем слое. В слоях трансформера 3 слоя были удалены целиком. Видно, что самый важный слой по сохранению голов / размерности FFN - восьмой. Это соотносится с важностью слоев для ASR из DistillHuBERT.
Дальше приводят ablations, которые показывают необходимость обрезания сверток, обучения из двух шагов и использования layer-to-layer дистилляции.

![Untitled](DPHuBERT%20Joint%20Distillation%20and%20Pruning%20of%20Self-Supervised%20Speech%20Models/Untitled%2012.png)

Также строят графики производительности дистиллированной модели для разных значений sparsity.

![Untitled](DPHuBERT%20Joint%20Distillation%20and%20Pruning%20of%20Self-Supervised%20Speech%20Models/Untitled%2013.png)

Из графиков следует, что для ASR, то есть для задачи, связанной с контентом, потери в качестве при уменьшении модели намного существеннее, чем для задач SID и IC.

И последняя табличка, где показаны результаты дистилляции Large-модели.

![Untitled](DPHuBERT%20Joint%20Distillation%20and%20Pruning%20of%20Self-Supervised%20Speech%20Models/Untitled%2014.png)

Видно, что на тех задачах, где HuBERT Large лучше HuBERT Base, DPHuBERT получается лучше чем HuBERT Base при таком же количестве параметров.