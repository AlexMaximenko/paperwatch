# Low-rank adaptation of large language model rescoring for parameter-efficient speech recognition

![Screenshot 2024-02-01 at 21.44.08.png](Low-rank%20adaptation%20of%20large%20language%20model%20rescoring/Screenshot_2024-02-01_at_21.44.08.png)

Ученые-исследователи искусственного интеллекта из Amazon предлагают особую методику файнтюна моделей для рескоринга - используя LORA. Файнтюнинг делают с помощью discriminative training objective + добавляют correlation-based regularization лосс.

В последнее время тяжелые transformer-модели достигли невероятных успехов в NLP. Чтобы выбивать из них максимум пользы при рескоринге на определенном домене - их нужно файнтюнить. Это может потребовать больших трат в плане железа + полный файнтюн может привести к переобучению и потери способности к обобщению на другие домены. Для решения этой проблемы хорошо подходит файнтюн с адаптерами - сама модель замораживается, а обучаются небольшое количество параметров, которые содержат эти самые адаптеры.

## LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS

Наверное самый попсовый и известный адаптер - LORA.

Его суть становится понятна при взгляде на картинку ниже:

![Screenshot 2024-02-01 at 22.02.17.png](Low-rank%20adaptation%20of%20large%20language%20model%20rescoring/Screenshot_2024-02-01_at_22.02.17.png)

Для каждой линейной проекции (конкретно в слоях трансформерных моделей это чаще всего относится к модулю Attention и рассматриваются матрицы $W_Q$, $W_K$, $W_V$ и $W_Q$) авторы предлагают обучать low-rank добавку, принимающую на вход и выдающую на выходе векторы тех же размерностей, что и оригинальная матрица. Маленький ранк и небольшое количество параметров этой добавки достигаются с помощью использования двух матриц, обозначаемых как $A\in \mathcal{R}^{d \times r }$ и $B\in \mathcal{R}^{r \times d }$.

Тогда результат слоя с этой добавкой будет получаться так:

$$
h = W_0x + \Delta Wx = W_0x + BAx
$$

В оригинальной матрице количество параметров — $d^2$, в обучаемых - $2rd$, где r обычно берется маленьким.

В процессе обучения оригинальные веса модели замораживаются и обучаются только веса адаптеров. Для инференса по линейнойсти можно просто обновить каждую матрицу $W_0$, добавив к ней матрицу $BA$, тем самым не создавая дополнительных вычислений.

Это выделяет подход на фоне других базовых подходов типа $residual \text{ } adapters$ и обучаемых промптов.

Кроме того, LORA выбивает лучшие метрики в сравнении с этими подходами и, как указано у них в статье, даже лучшие в сравнении с полным файнтюном.

Почему это работает?

Потому что, как показали авторы этой [статьи](https://arxiv.org/pdf/2012.13255.pdf), задача файнтюнинга предобученной модели может быть решена с высоким качеством в довольно небольшом подпространстве параметров.

### Measuring Intrinsic Dimension

Не совсем по теме, но показалось интересным, как авторы это показывали.

Чтобы понять интуицию “внутренней размерности задачи”, начнем с простого примера, как это делают [здесь](https://arxiv.org/pdf/1804.08838.pdf).

- Пусть у нас есть вектор параметров $\theta^{(D)} \in \mathcal{R}^D$ размерности $D$, случайную начальную инициализацию которой обозначим как $\theta_0^{(D)}$.
- Пусть $D=1000$ и хотим решить следущую задачу:
    - Первые 100 элементов вектора должны суммироваться в единицу
    - Вторые 100 элементов вектора должны суммироваться в 2
    - Дальше по аналогии
    
    Оптимизировать можем, например, Square Error.
    

Решением проблемы, как можно понять из линейной алгебры, будет 990-размерная гиперплоскость: из любой точки решения можно двинуть в 990 ортоногальных направлений и все еще получить решение. Обозначив за $s$ размерность решения, определим “внутреннюю размерность задачи” $d_{int}$ как размерность дополнения решения до всего пространства возможных значений параметра:

$$
D = d_{int} + s
$$

В случае нашей игрушечной задачи, внутренняя размерность будет равна $1000 - 990 = 10$.

Окей, мы смогли найти внутреннюю размерность для никому не нужной задачи. Что с нейронками и их задачами?

С нейронками будем искать эмпирически. 

- Пусть у нас снова есть вектор параметров $\theta^{(D)}$
- Хотим итеративно увеличивать число $d$ и проверять, является ли $d$ внутренней размерностью задачи
- Для этого будем обучать нейросеть градиентным спуском, но оптимизируя не напрямую $\theta^{(D)}$, а оптимизируя вектор $\theta^{(d)}$ размерности $d$, используя следующее выражение:
    
    $$
    \theta^{(D)} = \theta^{(D)}_0 + P\theta^{(d)},
    $$
    
    где $P$ - случайно инициализированная проекция, которую мы замораживаем и не обучаем, а $\theta^{(D)}_0$ - начальная инициализация модели (Xavier / He инициализация). 
    
    Столбцы $P$ нормируются к единице, таким образом шаг единичной нормы в нашем маленьком пространстве будет соответствовать шагу единичной нормы в исходном.
    
- Теперь будем итеративно увеличивать $d$з, обучать до сходимости и фиксировать, при какой размерности результат исходной модели достигает 90% от результата обучения полной модели. Сравнивают по конечной метрике для конкретной задачи: для классификации MNIST-а используют *accuracy,* для RL-тасок — reward.

Все графики, таблички и интересные замечания по поводу математики всего этого дела можно найти в статье, тут приведу только некоторые из них.

График для игрушечной задачи из примера

![Screenshot 2024-02-02 at 00.20.00.png](Low-rank%20adaptation%20of%20large%20language%20model%20rescoring/Screenshot_2024-02-02_at_00.20.00.png)

Графики для MNIST-а: на первом — FC-модели, на втором - Convolutional модели

![Screenshot 2024-02-02 at 00.22.12.png](Low-rank%20adaptation%20of%20large%20language%20model%20rescoring/Screenshot_2024-02-02_at_00.22.12.png)

![Screenshot 2024-02-02 at 00.22.24.png](Low-rank%20adaptation%20of%20large%20language%20model%20rescoring/Screenshot_2024-02-02_at_00.22.24.png)

Краткие выводы: 

- Низкая внутренняя размерность может объяснять возможность эффективноего сжатия моделей, например DistillBERT
- Для одной и той же задачи можно определять наиболее подходящую архитектуру модели, рассматривая отношения $D / d$ для разных размеров и разных архитектур

Теперь, с появившейся интуицией, можно взглянуть на чуть более актуальную [статью](https://arxiv.org/pdf/2012.13255.pdf), исследующую внутреннюю размерность задачи файнтюнинга предобученных языковых моделей.

Подход отличается некоторыми деталями:

- Используют не случайную матрицу $P$, а некую специальную матрицу
    
    $P = HGПHB$, на которую будет вычислительно проще домножать (в LM-ках много параметров, случайная матрица $P$ была бы слишком большой). Штука известна как Fastfood transform, сам за нее не шарю, желающие могут погуглить
    
- Используют послойную параметризацию и скейлинг:
    
    $$
    \theta_i^D = \theta_{0, i}^D + \lambda_iP(\theta^{d-m})_i
    $$
    
    Здесь каждая $\lambda_i$ - обучаемый коэффициент. Так авторы сохраняют важность layer-wise структуры трансформеров.
    

Первым делом проводят экспы для файнтюнинга разных версий BERT-а и RoBERT-ы на sentence prediction - датасетах.

![Screenshot 2024-02-02 at 00.37.38.png](Low-rank%20adaptation%20of%20large%20language%20model%20rescoring/Screenshot_2024-02-02_at_00.37.38.png)

Жестко, для 355-миллионной RoBERTa-Large для датасета MRPC качество 90% от полного файнтюна достигается в пространстве размерности 207. Как теперь спать после этого…

После этого, авторы выдвигают идею о том, что такая маленькая размерность вызвана тем, что предобученная модель уже очень круто знает NLP. И чем круче претрейн - тем меньше эта размерность для всех downstream-задач. Чтоб это подтвердить, они с нуля делают претрейн RoBERT-ы и каждые 10К обновлений претрейна находят $d_{90}$ для полученной туши. Ищут они на 6 downstream-сетах. Результаты поражают:

![Screenshot 2024-02-02 at 00.53.23.png](Low-rank%20adaptation%20of%20large%20language%20model%20rescoring/Screenshot_2024-02-02_at_00.53.23.png)

Все получается ровно так, как они и предсказали. Очень круто.

## Approach

### Second-pass rescoring

Что такое second-pass rescoring:

- Получаем N-best список гипотез бимсерча в декодере, к каждой из которой прилагается скор $s^a$
- Для каждой из них считаем скор $s^l$
- Финальный скор получаем как взвешенную сумму: $s = s^a + \beta \cdot s^l$

Как получить скор с помощью, например, BERT-а:

- докидываем к токенам гипотезы CLS-токен
- Прогоняем все это через модель и получаем $g_i = \operatorname{BERT}(E_i)$
- Прогоняем полученный вектор для CLS через FeedForwardNetwork и на выходе имеем скор: $s^l = \operatorname{FFNN}(g_i^{CLS})$

Почему вдруг BERT умеет таким образом считать скоры?

Потому что в качестве бейзлайна авторы использовали [RescoreBERT](https://arxiv.org/pdf/2202.01094.pdf).

### RescoreBERT

Тоже статья авторов из Amazon, в которой они приспосабливают BERT к задаче рескоринга. 

Первым делом они говорят, что считать Pseudo Log Likelihood (PLL) дорого.

Пусть $E = (e_1, \dots, e_{|E|})$ - последовательность токенов для рескоринга. Тогда для подсчета PLL нужно прогнать модель $|E|$ раз, каждый прогон маскируя один из токенов и считая для него вероятность по выходу модели:

$$
\operatorname{PLL}(E) = -\sum_{t=1}^{|E|} \log P(e_t | E_{\text{/} t}),
$$

где $E_{/t} = (\dots, e_{t-1}, [MASK], e_{t+1}, \dots)$.

Чтобы избавиться от необходимости прогонять модель столько раз, предлагают добавлять к токенам $CLS$-токен и дистиллировать $PLL$ в его выход, как это было сделано в статье [Masked Language Model Scoring](https://arxiv.org/pdf/1910.14659.pdf).

![Screenshot 2024-02-02 at 13.35.05.png](Low-rank%20adaptation%20of%20large%20language%20model%20rescoring/Screenshot_2024-02-02_at_13.35.05.png)

Вторым делом предлагают дообучать BERT с помощью дискриминативного обучения: берем аудио, прогоняем beam-search модели, достаем N-best лист и считаем один из следующих лоссов:

- Первый вариант - дообучение на MWER loss.
    
    Пусть $s_i$ - скоры гипотез после beam search, $\epsilon_i$ - число ошибок в словах (расстояние Левенштейна между гипотезой и референсом). Тогда:
    
    - Сначала бахнем софтмакс по скорам и получим вероятностное распределение на гипотезы: $P_i = \frac{e^{-s_i}}{\sum_{j=1}^{n}e^{-s_j}}$
    - Потом взвесим ошибки гипотез с этими вероятностями:
        
        $$
        \mathcal{L}_{MWER} = \sum_{i=1}^n P_i \cdot (\epsilon_i - \hat{\epsilon_H})
        $$
        
        Здесь для стабильности еще используется средняя ошибка $\hat{\epsilon_H} = \frac{1}{n} \sum_{i=1}^n\epsilon_i$.
        
- Второй вариант, придуманный авторами - **Matching word error distribution (MWED)**
    
    Его логика и цель - заставить скоры модели соответствовать $\epsilon_i$ - числу ошибок в словах. Оптимизация этого лосса, в пределе, даст нам идеальную модель для рескоринга. Считается как кросс-энтропия между софтмаксом скоров модели и софтмаксом по числу ошибок в гипотезах:
    
    $$
    d_i^{\epsilon} = \frac{e^{-\epsilon_i}}{\sum_{j=1}^{n}e^{-\epsilon_j}}
    $$
    
    $$
    d_i^{s} = \frac{e^{-s_i / T}}{\sum_{j=1}^{n}e^{-s_j / T}}
    $$
    
    $$
    \mathcal{L}_{MWED} = \sum_{i=1}^n d_i^{\epsilon}\log d_i^s
    $$
    
    Дальше проводят экспы и исследуют важность совместного обучения дистилляции + дискриминативного обучения, адаптации к домену. 
    
    В качестве LM-ок используют разные BERT-ы (opensource для Либриспича, внутренние - для внутренних сетов).
    
    ![Screenshot 2024-02-02 at 13.15.10.png](Low-rank%20adaptation%20of%20large%20language%20model%20rescoring/Screenshot_2024-02-02_at_13.15.10.png)
    
    Результаты получились вполне хорошими.
    

С бейзлайном разобрались, возвращаемся к статье.

Хоть LORA-файнтюнинг и помогает бороться с переобучением под домен, авторы пишут, что до конца проблема не решается.

Поэтому для файнтюна исползьуют не только MWER-training, описанный в части про RescoreBERT, но и дополнительный correlation-based лосс:

$$
\mathcal{L} = \mathcal{L}_{MWER} +\lambda \mathcal{L}_{cor},
$$

где $\mathcal{L}_{cor} = \|\Sigma - I\|$. 

$I$ здесь - единичная матрица, $\Sigma$ - матрица корреляций, в которой $\Sigma_{ij}$ есть корреляция Пирсона между $i$-ой и $j$-ой строкой эмбеддинга CLS-токена $g^{CLS}$.

![Screenshot 2024-02-02 at 13.57.25.png](Low-rank%20adaptation%20of%20large%20language%20model%20rescoring/Screenshot_2024-02-02_at_13.57.25.png)

## Experiments

В своих экспериментах по адаптации модели на новые домены, авторы варьируют следующие параметры:

- Внутренняя размерность LORA-параметров — рассматривают значения $[4, 8, 16, 32]$
- Рассматривают применение LORA к двум сетам слоев:
    - *Key* и *Value* матрицы в аттеншне: $[W_q, W_v]$
    - Key, Query и Value матрицы в аттеншне и матрицы в Feed-forward слоях: $[W_q, W_k, W_v, W_{f_1}, W_{f_2}]$

Для экспериментов с Либриспичом файнтюнят $cased \text{ } BERT_{base}$ для честного сравнения с предыдущими работами.

Для внутренних сетов файнтюнят $rescoreBERT$.

### Datasets

- *Librispeech* для сравнения с предыдущими работами
- Внутренние сеты *Messaging* (350 часов) *и Music* (150 часов)
- Также исследуют scaling law применительно к предобученной модели и данным, используя внутренние *conversational domain* данные
- Для валидации исползуют 3 out-of-domain внутренних сета: *General* (194 часа) *, Shopping* (20 часов) и  *Knowledge* (5 часов)

Свой подход сравнивают с:

- Residual Adapter
- BitFit (тюнинг bias-ов всех слоев)
- Prefix (prompt - tuning)
- Полным файнтюнингом

### Results

Сначала показывают результаты эксперимента с файнтюнингом на внутренний сет

![Screenshot 2024-02-02 at 14.21.16.png](Low-rank%20adaptation%20of%20large%20language%20model%20rescoring/Screenshot_2024-02-02_at_14.21.16.png)

Видно, что их метод получает лучшие результаты как на Target-домене, так и на Out-Of-Domain сетах, в то время как все остальные методы деградируют на OOD.

В качестве акустики здесь используют внутреннюю RNN-T модель.

Затем показывают результаты на Либриспиче:

![Screenshot 2024-02-02 at 14.22.57.png](Low-rank%20adaptation%20of%20large%20language%20model%20rescoring/Screenshot_2024-02-02_at_14.22.57.png)

Здесь нет OOD-сетов, но их водход показывает лучшие результаты по сравнению с другими Parameter-Efficient методами, достигая тех же метрик, что и полный файнтюн.

В качестве акустики здесь используют Whisper.

Затем приводят пару доказательств того, что их подход учить сильно дешевле и проще, чем полный файнтюнинг.

На первом графике показано время обучения LORB-подхода и полного файнтюнинга.

![Screenshot 2024-02-02 at 14.26.06.png](Low-rank%20adaptation%20of%20large%20language%20model%20rescoring/Screenshot_2024-02-02_at_14.26.06.png)

На втором - стабильность обучения по отношению к гиперпараметрам **Warmup Steps** и Learning Rate.

![Screenshot 2024-02-02 at 14.27.31.png](Low-rank%20adaptation%20of%20large%20language%20model%20rescoring/Screenshot_2024-02-02_at_14.27.31.png)

### Large Language Model Scaling Law Results

В последней части своей работы исследуют робастность их метода по отношению к размеру обучаемой модели и количеству данных.

Для этого у них есть внутренний сет из Conversation-домена и 3 внутренних предобученных RescoreBERT-а: на 5M, 170M и 1B параметров.

Сначала берут 150К уттерансов для обучения и варьируют размер модели:

![Screenshot 2024-02-02 at 14.33.58.png](Low-rank%20adaptation%20of%20large%20language%20model%20rescoring/Screenshot_2024-02-02_at_14.33.58.png)

Здесь получается замер на IN-DOMAIN тестовом сете и видно, что на маленьких моделях полный файнтюнинг получается сильно лучше, но для 1В модели подход с адаптерами уже побеждает, пусть и несильно.

Затем они фиксируют модель (1B RescoreBERT) и меняют количество данных, на которых он дообучается:

![Screenshot 2024-02-02 at 14.38.07.png](Low-rank%20adaptation%20of%20large%20language%20model%20rescoring/Screenshot_2024-02-02_at_14.38.07.png)

Видно, что LORB-подход насыщается, а полный файнтюнинг продолжает улучшать качество - что логично.

На последнем графике показывают то же самое, но добавляют эксперименты с различными внутренними размерностями адаптеров:

![Screenshot 2024-02-02 at 14.40.41.png](Low-rank%20adaptation%20of%20large%20language%20model%20rescoring/Screenshot_2024-02-02_at_14.40.41.png)

## Conclusions

Представленный авторами подход работает круто в сравнении с остальными Parameter-Efficient методами. 

Если сравнивать с полным файнтюнингов - тут зависит от размера модели и датасета. К сожалению, они не указывают на чем были предобучены их внутренние модели и не совсем понятно, почему до серии экспериментов с **Large Language Model Scaling Laws** адаптеры работают лучше как в домене, так и на Out-Of-Domain, а потом полный файнтюнинг становится значительно лучше на всех размерах датасета и со всеми моделями (пусть и в домене).