# PROMPTASR FOR CONTEXTUALIZED ASR WITH CONTROLLABLE STYLE

[https://arxiv.org/pdf/2309.07414.pdf](https://arxiv.org/pdf/2309.07414.pdf)

Авторы из xiaomi выпустили статью с датасетом Libriheavy, который содержит аудио с транскрипциями и с контекстами и решили сразу же выпустить статью, в которой они обучили ASR-модель, которая умеет использовать контексты. Кроме того, они последовали моде и сделали свою модель end-to-end (с капитализацией и пунктуацией т.к. транскрипции в Libriheavy имеют необходимую разметку.

### Модель

Архитектура простая. Берем текстовый энкодер (в их случае - BERT), аудио-энкодер (в их случае - Zipformer) и добавляем в трансформерные слои аудио-энкодера кросс-аттеншн на аутпут текстового энкодера. Снизу картинка.

![Screenshot 2023-09-22 at 15.16.08.png](PROMPTASR%20FOR%20CONTEXTUALIZED%20ASR%20WITH%20CONTROLLABLE%20STYLE/Screenshot_2023-09-22_at_15.16.08.png)

В текстовый энкодер мы подаем две последовательности токенов: $P_S$ - **style prompt** и $P_C$ - **context prompt**. 
**Style prompt** отвечает за стиль аутпута нашей модели. В статье рассматривали 2 варианта стиля: Upper-Cased without punctuation (UC) и MixedCased with Punctuation (MCP). 

**Сontext prompt** отвечает за контекст. Им может быть предшествующий сэмплу текст (например, при распознавании длинного аудио можно подавать результат распознавания на предыдущих уттерансах чтобы улучшить распознавание на текущем).

Важно отметить, что в обучении они не меняли стиль **context prompt** так, чтобы он соответствовал **style prompt.** Менялся только референс. Пример в табличке ниже.

![Screenshot 2023-09-22 at 15.22.06.png](PROMPTASR%20FOR%20CONTEXTUALIZED%20ASR%20WITH%20CONTROLLABLE%20STYLE/Screenshot_2023-09-22_at_15.22.06.png)

Также чтобы помочь аудио-энкодеру отличать **style prompt** от **context prompt** авторы добавляют обучаемый вектор $v$ к токенам, получившимся после декодирования **style prompt-**а. Для нагядности положим $P_c = P_{c,1},\dots P_{c,n}$ - токены **context prompt**-а, $P_c = P_{s,1},\dots P_{s,m}$ - токены **style prompt-**а, $X$ - входное аудио. Тогда модель работает следующим образом:

$$
\mathcal{E}_c = Enc^T(P_c); \\
\mathcal{E}_s = Enc^T(P_s) + v; \\
\mathcal{G} = Enc^A(Concat(\mathcal{E}_c, \mathcal{E}_s), X) \\
y = Dec^A(G)
$$

Декодером здесь может быть что угодно (CTC, RNNT, LAS).

### Данные

Для обучения брали 2 сетапа: 

1. 5000 часов данных из Libriheavy, где для каждого сэмпла имеется транскрипция с капитализацией и пунктуацией + контекст (текст, предшествующией этому сэмплу) длиной 1000 байт.
2. 2000 записанных обсуждений / подкастов, полученных с National Public Radio (NPR). В этом датасете также были транскрипция с капитализацией и пунктуацией + контекст длиной 1000 байт

### Конструирование промптов

Важно написать как строились промпты к тренировочным записям. Как уже упомяналось, использовали 2 типа style promts: Upper-Cased without punctuation (UC) and MixedCased with Punctuation (MCP). Чтобы подать стиль на вход модели, для каждого сэмпла из батча они брали небольшой кусок content prompt-а из другого уттеранса в этом же батче и приводили его к нужному стилю. Стиль выбирался случайно, причем вероятность стиля с капитализацией и нормализацией была выше (0.7) чем у нормализованного стиля (0.3).

В качестве context prompt авторы равновероятно выбирали либо 1000 байт предшествующего текста, либо список слов, который составляли следующим образом:
Сначала отсортировали все слова в трейне по встречаемости и обозвали редкими слова с 10000 места и выше. После этого, они выбирали все редкие слова, встречающиеся в самом сэмпле, и накидывали к ним 50-100 дистракторов (просто рандомно выбранных слов).

Также, авторы пишут что использовали две следующие регуляризации чтобы сделать модель более робастной к сетапам, когда нет промпта вообще (ни контекста, ни стиля) и когда контекст вообще не соответствует аудио:

1. С маленькой вероятностью убирали весь промпт
2. С маленькой вероятностью меняли **context prompt** у сэмплов в батче

### Метрики

Везде замеряли WER.