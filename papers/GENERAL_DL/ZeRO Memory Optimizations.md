# ZeRO: Memory Optimizations Toward Training Trillion Parameter Models

[https://arxiv.org/pdf/1910.02054.pdf](https://arxiv.org/pdf/1910.02054.pdf)

[https://www.notion.so](https://www.notion.so)

Исследователи из **Microsoft** предлагают очень крутой подход к оптимизации распределенного обучения больших и супербольших моделек.

## Distributed training

Для обучения моделек в multi-gpu сетапе существует несколько подходов, использование которых зависит от размера модели. 
Самые базовые/популярные из них:

- Data Parallel
- ~~Model~~   Pipeline Parallel
- Tensor Parallel

### Data Parallel

База, используется везде и всеми для обучения моделек адекватных размеров.

Пусть у нас имеется N видеокарт. Тогда цикл обучения состоит из следующих итераций:

1. Создаем N процессов с репликами модели, по одному на каждую видеокарту
2. Делим батч данных на N частей, и каждый процесс делает **.forward**  и **.backward** своей части
3. Процессы синхронизируются между собой, с помощью операции **AllReduce** усредняют градиенты и обновляют свои параметры
    
    ![Снимок экрана 2024-03-29 в 14.33.37.png](ZeRO%20Memory%20Optimizations/screenshot_14.33.37.png)
    

Все. От обучение на одной ноде отличается только необходимостью синхронизировать градиенты после каждого батча.

![Untitled](ZeRO%20Memory%20Optimizations/Untitled.png)

![Untitled](ZeRO%20Memory%20Optimizations/Untitled%201.png)

Этот подход используется практически всегда, если ваша модель влезает с батчом адекватного размера на 1 видеокарту. Используя фреймворки для обучения (pytorch lightning, transformers) можно превращать свой код для обучения модели на 1 гпу в код для распределенного обучения просто добавлением **strategy=ddp** в тренера (еще стоит проследить за distributed-сэмплером, но вроде как эти фреймворки умеют почти безболезненно оборачивать дефолтные сэмплеры в distributed-формат).

Но что делать, если ты LLM-щик и твоя модель не влезает? Использовать подход **Pipeline Parallel.**

### ~~Model~~ Pipeline Parralel

В **Data Parallel** мы поделили наши данные по разным видеокартам, на каждой из которых лежал инстанс нашей модели. Если модель слишком большая и не влезает на 1 карту, мы можем разрезать нашу модель на куски, разложить куски по разным картам и обрабатывать каждый батч данных, поочередно проходясь по картам, содержащим разные куски модели. Снизу картинка.

![Untitled](ZeRO%20Memory%20Optimizations/Untitled%202.png)

Последовательность будет следующая:

1. Режем модель на куски по вертикали (Например, на gpu0 кладем первые 4 трансформерных слоя, на gpu1 - с 5 по 8 и тд.)
2. Загружаем наш батч данных на gpu0 и делаем **.forward**
3. Выход из последнего слоя на карте gpu0 мы передаем на вход первому слою на карте gpu1 и тоже делаем .**forward**
4. Продолжаем процесс, пока не сделаем **.forward** последнего слоя на последней **gpu**
5. Теперь время делать **.backward,** причем делается он с конца. Делаем **.backward** на последней gpu, потом на предпоследней и тд. пока не дойдем до первой.
6. Радуемся, **backward** закончился и градиенты получены. Теперь можно обновить параметры и проделать то же самое заново

В этом подходе есть очень большая проблема - в каждый момент времени делом занята только одна видеокарта, остальные же простаивают. Иллюстрацию можно видеть на первой схеме на картинке ниже:

![Untitled](ZeRO%20Memory%20Optimizations/Untitled%203.png)

Немного уменьшить масштаб проблемы позволяет переход к **Pipeline Parallelism,** когда мы делим наш батч на несколько микробатчей и используем ГПУ конкурентно (схема 2 на картинке выше). Проблема простаивания видеокарт хоть и не решается полностью, но все равно сильно повышает утилизацию и ускоряет время обучения.

Проблемы и сложности подхода:

1. Для **Pipeline Parallelism** нужно будет тюнить параметр, отвечающий за гранулярность разбиения батча
2. Придется основательно разбираться в поддерживающих этот подход фреймворках и их ограничениях. С большой вероятностью придется менять код модели.
3. Дополнение к 2 - если вам нужно передавать данные из одного слоя в другой (например, cross attention) - будет больно.
4. Все еще простаивание gpu

Окей, здесь мы побили модель вертикально. Может быть, есть способ разбить ее горизонтально?

Есть - **Tensor Parallelism**

### Tensor Parallelism

В этом подходе предлагается делить не разные слои на разные карты, а делить некоторые слои между картами горизонтально.

В самом начале сразу следует оговориться, что практически все фреймворки для обучения больших моделей с помощью **Pipeline/Tensor Parallelism** заточены под работу с трансформерами по понятным причинам.

Основная составляющая трансформеров - линейный слой, который представляет из себя перемножение матриц. Эту операцию можно побить на две (и более), как это показано на картинке ниже:

![Untitled](ZeRO%20Memory%20Optimizations/Untitled%204.png)

С помощью такого прикола (разбитию по колонкам) можно легко распараллелить MLP-слой в трансформере и проводить синхронизацию только в конце слоя. Ниже картинка:

![Untitled](ZeRO%20Memory%20Optimizations/Untitled%205.png)

Аналогично можно параллелить и Attention-слой:

![Untitled](ZeRO%20Memory%20Optimizations/Untitled%206.png)

У подхода есть серьезное ограничение - он будет работать эффективно только при высокой пропускной способности сети между нодами, по которым мы делим вычисления. Из-за этого с текущим железом деление можно проводить только внутри одной ноды.

Крутой подход, который позволяет обучать большие модельки и не приводит к сильной недоутилизации ресурсов, но все еще требует тыкаться в код + подбирать оптимальные параметры деления.

## ZeRO Intro

Итак, мы рассмотрели 3 подхода:

- **DDP —** кажется самым простым в реализации и самым эффективным в плане утилизации железа.
- **Model Parallel -** позволяет обучать самые большие модельки ценой недоутилизации ресурсов
- **Tensor Parallel -** позволяет обучать очень большие модельки и не терять так много ресурсов, как в подходе **Model Parallel**

Как всегда, хочется сделать суперподход, объединяющий все их достоинства, и не имеющий никаких недостатков.

Это постарались сделать исследователи из Microsoft и, наверное, можно сказать что у них это получилось.

Прежде чем переходить к самому **ZeRO** надо бы вспомнить, на что же вообще уходит память gpu при обучении модели, у которой **N** параметров с помощью [mixed precision](https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/) и оптимизатора Adam (классический сетап):

1. Веса модели + градиенты в fp16: 2N + 2N = 4N
2. Состояния оптимизатора: fp32 веса + fp32 первый момент + fp32 второй момент = 4N * 3 = 12N
3. Активации, сохраняющиеся при **forward-**е для использования в **backward-**е
4. Какие-либо промежуточные тензоры, появляющиеся в **forward-**е ненадолго и затем удаляющиеся

Итого, для модели размера N мы получаем необходимость держать на видеокарте ее состояние размера 16N байт + активации + промежуточные стейты. Итого, для модели размером 1.5В параметров мы получаем 24Гб только за счет состояния, что уже очень много. В статье авторы приводят пример размера активаций для 1.5В модели GPT-2:

С sequence_length = 1K и batch_size=32 ее активации весят примерно 60Гб. Уже не влезаем даже на А100.

Еще авторы пишут о двух следующих приколах / проблемах:

- Temporary buffers - какие-то промежуточные буферы, которые могут весить непредсказуемое число Гигабайт. Как пример приводят ситуацию, когда для оптимизации коммуникаций перед усреднением градиента каждый процесс складывает fp16- градиенты в fp32 flattened fused тензор перед обменом, что также занимает доп. память
- Memory fragmentation - фрагментация памяти ГПУ, которая приводит к ошибке CUDA OUT OF MEMORY даже при наличии нужного количества памяти на ГПУ (в этом случае память располагается непоследовательно)

**ZeRO** собирается решать все эти проблемы:

- оптимизировать хранение состояний модели и поделить его между gpu, не увеличив при этом объем коммуникаций и не замедлив обучение
- решить проблемы с temporary buffers, memory fragmentation и еще одну, связанную с model-parallel обучением (в следующей серии)

## ZeRO-DP

В этой части рассмотрим оптимизации, касающиеся хранения состояний модели.

![Снимок экрана 2024-03-29 в 15.32.35.png](ZeRO%20Memory%20Optimizations/screenshot_15.32.35.png)

В целом, понять суть подхода можно просто взглянув на картинку выше.

**ZeRO-DP** имеет три стадии / ступени оптимизации:

1. Optmizer State Partitioning
2. Gradient Partitioning
3. Parameter Partitioning

### Optimizer Partitioning

На первом этапе авторы предлагают поделить хранение состояний оптимизатора между всеми воркерами.

Пусть у нас есть K видеокарт. Давайте разобьем параметры модели на K равных частей и каждый из воркеров будет хранить состояния оптимизатора только для нужных ему частей.

 Соответственно, обновлять каждый воркер тоже будет только свою малую часть параметров. 

В конце каждого шага обучения с помощью операции **AllGather** процессы отошлют друг другу обновленные части модели и на выходе будут иметь одинаковые и обновленные веса.

![Снимок экрана 2024-03-29 в 15.39.49.png](ZeRO%20Memory%20Optimizations/screenshot_15.39.49.png)

Этот этап при больших K позволит нам уменьшить размер состояний модели в 4 раза:

$$
4N + 12N \rightarrow 4N + \frac{12N}{K} \approx 4N
$$

Огонь, идем дальше

### Gradient Partitioning

Раз у нас каждый процесс обновляет только свою часть параметров, синхронизированные градиенты ему нужны тоже только для этой части. Соответственно, когда для очередного слоя во время **backward** у нас появляются градиенты - мы делаем их reduce scatter на ответственный за эту часть параметров процесс. На всех остальных процессах после reduction-а эти градиенты могут быть высвобождены т.к. больше не нужы.

Эффективно это выходит ReduceScatter операция, размазанная во времени.

![Снимок экрана 2024-03-29 в 15.59.40.png](ZeRO%20Memory%20Optimizations/screenshot_15.59.40.png)

# Part 2: Model Parallel part and analysis

## ZeRO-DP-stage-3

Последней оптимизацией DP-обучения в семействе ZeRO-DP является **stage-3.** Здесь авторы предлагают распределять по девайсам еще и параметры модели.

Как работает:

- Передаем на каждого воркера батч данных (батчи разные, как в обычном DDP)
- Воркер под номером 1 делает broadcast своих параметров всем остальным воркерам чтобы сделать forward первой части модели (той части, параметры которой хранит первый воркер)
- Все воркеры делают forward первой части, затем удаляют параметры этой части (за исключением воркера 1, он их хранит)
- Воркер под номером 2 делает broadcast своих параметров всем остальным воркерам
- …
- Доходим до backward, где происходит то же самое, но в обратном порядке

Видео в блоге DeepSpeed помогает понять:

[www.microsoft.com](https://www.microsoft.com/en-us/research/uploads/prod/2020/02/Turing-Animation.mp4?_=1)

Авторы говорят, что объем коммуникаций в этом случае увеличавается на 50%, про замедление forward и backward из-за ожидания коммуникаций они не пишут. Думаю коммуникация происходит заранее (во время подсчета предыдущего слоя), но в этом случае увеличивается расход памяти. И даже в таком случае ZeRO-stage-3 все равно будет очень неэффективен в multi-node сетапе.

Поэтому авторы для экспериментов с большими модельками используют его в связке и Pipeline Parallel и Tensor Parallel.

## ZeRO-R

Помимо ZeRO-DP, авторы также закодили эффективное обращение с памятью и партицирование активаций во время Tensor Parallel-обучения.

### **Partitioned Activation Checkpointing**

Стандартный Tensor-Parallel подход из статьи Megatron-LM имеет недостаток (если смотреть с ракурса занимаемой памяти) - в нем воркеры хранят одни и те же активации целиком. Авторы предлагают разбивать активацию и хранить ее по кускам на воркерах, делая allgather на этапе backward-а, когда она нужна. Более того, для особенно больших моделей авторы добавили возможность выгружать эти активации на CPU, тем самым дополнительно освобождая память

### **Constant Size Buffers**

Как было описано ранее, при обучении память на GPU могут занимать временные буфферы, которые нужны, например, для загрузки в них всех параметров и выполнения коммуникаций (пишут, что обычно эти буфферы представляют из себя непрерывную область памяти и параметры туда сохраняются в fp32). Авторы же предлагают заранее создать performance-efficient буффер константного размера и использовать только его.

### Memory Defragmentation

Здесь авторы борятся с проблемой фрагментации памяти, вызванной долго- и коротко-живущими тензорами. К долгоживущим относят параметры, градиенты и активации (которые мы не дропаем для дальнейшего пересчета),  к короткоживущим - активации, дропающиеся для переподсчета во время backward, активации градиентов и различные буфферы. Дефрагментации достигают за счет выделения специальной непрерывной области памяти для долгоживущих тензоров (что также ускоряет время на работу аллокатора памяти

Полезные ссылки:

[**ZeRO & DeepSpeed: New system optimizations enable training models with over 100 billion parameters**](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)

[**DeepSpeed: Extreme-scale model training for everyone**](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)

[Megatron-2 NVIDIA](https://arxiv.org/pdf/2104.04473.pdf)